{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest,SelectPercentile,SelectFromModel,chi2,f_classif,mutual_info_classif,RFE\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.svm import SVC,LinearSVC,LinearSVR,SVR\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('tech_factors.csv')\n",
    "columns = data.columns\n",
    "data_x = data[columns[2:]]\n",
    "data_y = data[columns[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\wjb\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:7620: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "d:\\Users\\wjb\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "def winsorize_and_standarlize(data,qrange=[0.05,0.95],axis=0):\n",
    "    '''\n",
    "    input:\n",
    "    data:Dataframe or series,输入数据\n",
    "    qrange:list,list[0]下分位数，list[1]，上分位数，极值用分位数代替\n",
    "    '''\n",
    "    if isinstance(data,pd.DataFrame):\n",
    "        if axis == 0:\n",
    "            q_down = data.quantile(qrange[0])\n",
    "            q_up = data.quantile(qrange[1])\n",
    "            index = data.index\n",
    "            col = data.columns\n",
    "            for n in col:\n",
    "                data[n][data[n] > q_up[n]] = q_up[n]\n",
    "                data[n][data[n] < q_down[n]] = q_down[n]\n",
    "            data = (data - data.mean())/data.std()\n",
    "            data = data.fillna(0)\n",
    "        else:\n",
    "            data = data.stack()\n",
    "            data = data.unstack(0)\n",
    "            q = data.quantile(qrange)\n",
    "            index = data.index\n",
    "            col = data.columns\n",
    "            for n in col:\n",
    "                data[n][data[n] > q[n]] = q[n]\n",
    "            data = (data - data.mean())/data.std()\n",
    "            data = data.stack().unstack(0)\n",
    "            data = data.fillna(0)\n",
    "            \n",
    "    elif isinstance(data,pd.Series):\n",
    "        name = data.name\n",
    "        q = data.quantile(qrange)\n",
    "        data[data>q] = q\n",
    "        data = (data - data.mean())/data.std()\n",
    "    return data\n",
    "data_x = winsorize_and_standarlize(data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA降维\n",
    "def pca_analysis(data,n_components='mle'):\n",
    "    index = data.index\n",
    "    model = PCA(n_components=n_components)\n",
    "    model.fit(data)\n",
    "    data_pca = model.transform(data)\n",
    "    df = pd.DataFrame(data_pca,index=index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelection():\n",
    "    '''\n",
    "    特征选择：\n",
    "    identify_collinear：基于相关系数，删除小于correlation_threshold的特征\n",
    "    identify_importance_lgbm：基于LightGBM算法，得到feature_importance,选择和大于p_importance的特征\n",
    "    filter_select:单变量选择，指定k,selectKBest基于method提供的算法选择前k个特征，selectPercentile选择前p百分百的特征\n",
    "    wrapper_select:RFE，基于estimator递归特征消除，保留n_feature_to_select个特征\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.supports = None #bool型，特征是否被选中\n",
    "        self.columns = None  #选择的特征\n",
    "        self.record_collinear = None #自相关矩阵大于门限值\n",
    "        \n",
    "    def identify_collinear(self, data, correlation_threshold):\n",
    "        \"\"\"\n",
    "        Finds collinear features based on the correlation coefficient between features. \n",
    "        For each pair of features with a correlation coefficient greather than `correlation_threshold`,\n",
    "        only one of the pair is identified for removal. \n",
    "\n",
    "        Using code adapted from: https://gist.github.com/Swarchal/e29a3a1113403710b6850590641f046c\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "\n",
    "        data : dataframe\n",
    "            Data observations in the rows and features in the columns\n",
    "\n",
    "        correlation_threshold : float between 0 and 1\n",
    "            Value of the Pearson correlation cofficient for identifying correlation features\n",
    "\n",
    "        \"\"\"\n",
    "        columns = data.columns\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "\n",
    "        # Calculate the correlations between every column\n",
    "        corr_matrix = data.corr()\n",
    "        \n",
    "        self.corr_matrix = corr_matrix\n",
    "    \n",
    "        # Extract the upper triangle of the correlation matrix\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "        # Select the features with correlations above the threshold\n",
    "        # Need to use the absolute value\n",
    "        to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n",
    "        obtain_columns = [column for column in columns if column not in to_drop]\n",
    "        self.columns = obtain_columns\n",
    "        # Dataframe to hold correlated pairs\n",
    "        record_collinear = pd.DataFrame(columns = ['drop_feature', 'corr_feature', 'corr_value'])\n",
    "\n",
    "        # Iterate through the columns to drop\n",
    "        for column in to_drop:\n",
    "\n",
    "            # Find the correlated features\n",
    "            corr_features = list(upper.index[upper[column].abs() > correlation_threshold])\n",
    "\n",
    "            # Find the correlated values\n",
    "            corr_values = list(upper[column][upper[column].abs() > correlation_threshold])\n",
    "            drop_features = [column for _ in range(len(corr_features))]    \n",
    "\n",
    "            # Record the information (need a temp df for now)\n",
    "            temp_df = pd.DataFrame.from_dict({'drop_feature': drop_features,\n",
    "                                             'corr_feature': corr_features,\n",
    "                                             'corr_value': corr_values})\n",
    "\n",
    "            # Add to dataframe\n",
    "            record_collinear = record_collinear.append(temp_df, ignore_index = True)\n",
    "\n",
    "        self.record_collinear = record_collinear\n",
    "        return data[obtain_columns]\n",
    "     \n",
    "        \n",
    "    def identify_importance_lgbm(self, features, labels,p_importance=0.8, eval_metric='auc', task='classification', \n",
    "                                 n_iterations=10, early_stopping = True):\n",
    "        \"\"\"\n",
    "        \n",
    "        Identify the features with zero importance according to a gradient boosting machine.\n",
    "        The gbm can be trained with early stopping using a validation set to prevent overfitting. \n",
    "        The feature importances are averaged over n_iterations to reduce variance. \n",
    "        \n",
    "        Uses the LightGBM implementation (http://lightgbm.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "        Parameters \n",
    "        --------\n",
    "        features : dataframe\n",
    "            Data for training the model with observations in the rows\n",
    "            and features in the columns\n",
    "\n",
    "        labels : array, shape = (1, )\n",
    "            Array of labels for training the model. These can be either binary \n",
    "            (if task is 'classification') or continuous (if task is 'regression')\n",
    "            \n",
    "        p_importance:float, range[0,1],default = 0.8\n",
    "            sum of the importance of features above the value\n",
    "\n",
    "        eval_metric : string\n",
    "            Evaluation metric to use for the gradient boosting machine\n",
    "\n",
    "        task : string, default = 'classification'\n",
    "            The machine learning task, either 'classification' or 'regression'\n",
    "\n",
    "        n_iterations : int, default = 10\n",
    "            Number of iterations to train the gradient boosting machine\n",
    "            \n",
    "        early_stopping : boolean, default = True\n",
    "            Whether or not to use early stopping with a validation set when training\n",
    "        \n",
    "        \n",
    "        Notes\n",
    "        --------\n",
    "        \n",
    "        - Features are one-hot encoded to handle the categorical variables before training.\n",
    "        - The gbm is not optimized for any particular task and might need some hyperparameter tuning\n",
    "        - Feature importances, including zero importance features, can change across runs\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # One hot encoding\n",
    "        data = features\n",
    "        features = pd.get_dummies(features)\n",
    "\n",
    "        # Extract feature names\n",
    "        feature_names = list(features.columns)\n",
    "\n",
    "        # Convert to np array\n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels).reshape((-1, ))\n",
    "\n",
    "        # Empty array for feature importances\n",
    "        feature_importance_values = np.zeros(len(feature_names))\n",
    "        \n",
    "        print('Training Gradient Boosting Model\\n')\n",
    "        \n",
    "        # Iterate through each fold\n",
    "        for _ in range(n_iterations):\n",
    "\n",
    "            if task == 'classification':\n",
    "                model = lgb.LGBMClassifier(n_estimators=100, learning_rate = 0.05, verbose = -1)\n",
    "\n",
    "            elif task == 'regression':\n",
    "                model = lgb.LGBMRegressor(n_estimators=100, learning_rate = 0.05, verbose = -1)\n",
    "\n",
    "            else:\n",
    "                raise ValueError('Task must be either \"classification\" or \"regression\"')\n",
    "                \n",
    "            # If training using early stopping need a validation set\n",
    "            if early_stopping:\n",
    "                \n",
    "                train_features, valid_features, train_labels, valid_labels = train_test_split(features, labels, test_size = 0.15)\n",
    "\n",
    "                # Train the model with early stopping\n",
    "                model.fit(train_features, train_labels, eval_metric = eval_metric,\n",
    "                          eval_set = [(valid_features, valid_labels)],\n",
    "                           verbose = -1)\n",
    "                \n",
    "                # Clean up memory\n",
    "                gc.enable()\n",
    "                del train_features, train_labels, valid_features, valid_labels\n",
    "                gc.collect()\n",
    "                \n",
    "            else:\n",
    "                model.fit(features, labels)\n",
    "\n",
    "            # Record the feature importances\n",
    "            feature_importance_values += model.feature_importances_ / n_iterations\n",
    "\n",
    "        feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "\n",
    "        # Sort features according to importance\n",
    "        feature_importances = feature_importances.sort_values('importance', ascending = False).reset_index(drop = True)\n",
    "\n",
    "        # Normalize the feature importances to add up to one\n",
    "        feature_importances['normalized_importance'] = feature_importances['importance'] / feature_importances['importance'].sum()\n",
    "        feature_importances['cumulative_importance'] = np.cumsum(feature_importances['normalized_importance'])\n",
    "        select_df = feature_importances[feature_importances['cumulative_importance']<=p_importance]\n",
    "        select_columns = select_df['feature']\n",
    "        self.columns = list(select_columns.values)\n",
    "        res = data[self.columns]\n",
    "        return res\n",
    "        \n",
    "    def filter_select(self, data_x, data_y, k=None, p=50,method=f_classif):\n",
    "        columns = data_x.columns\n",
    "        if k != None:\n",
    "            model = SelectKBest(method,k)\n",
    "            res = model.fit_transform(data_x,data_y)\n",
    "            supports = model.get_support()\n",
    "        else:\n",
    "            model = SelectPercentile(method,p)\n",
    "            res = model.fit_transform(data_x,data_y)\n",
    "            supports = model.get_support()\n",
    "        self.support_ = supports\n",
    "        self.columns = columns[supports]\n",
    "        return res\n",
    "    \n",
    "    def wrapper_select(self,data_x,data_y,n,estimator):\n",
    "        columns = data_x.columns\n",
    "        model = RFE(estimator=estimator,n_features_to_select=n)\n",
    "        res = model.fit_transform(data_x,data_y)\n",
    "        supports = model.get_support() #标识被选择的特征在原数据中的位置\n",
    "        self.supports = supports\n",
    "        self.columns = columns[supports]\n",
    "        return res\n",
    "    \n",
    "    def embedded_select(self,data_x,data_y,estimator,threshold=None):\n",
    "        columns = data_x.columns\n",
    "        model = SelectFromModel(estimator=estimator,prefit=False,threshold=threshold)\n",
    "        res = model.fit_transform(data_x,data_y)\n",
    "        supports = model.get_support()\n",
    "        self.supports = supports\n",
    "        self.columns = columns[supports]\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gradient Boosting Model\n",
      "\n",
      "['MAWR', 'BIAS', 'ACCER', 'MTR', 'KBJ', 'CYE', 'RSI', 'WR', 'CCI', 'MASS', 'open', 'MAMASS', 'ADTM', 'volume', 'money', 'AR', 'close']\n",
      "         MAWR      BIAS     ACCER       MTR       KBJ       CYE       RSI  \\\n",
      "0    1.121323 -2.385210 -2.296275  2.775741 -1.244532 -2.273093 -1.800326   \n",
      "1    0.598230 -2.341578 -2.296275  1.739049 -1.205471 -2.273093 -1.086293   \n",
      "2    1.679410 -2.385210 -2.296275  2.775741 -1.460708 -2.273093 -1.800326   \n",
      "3    1.092997 -2.385210 -2.296275  2.775741 -1.515147 -2.273093 -1.453523   \n",
      "4    1.679410 -2.385210 -2.296275  2.775741 -1.640033 -2.273093 -1.800326   \n",
      "5    1.381441 -2.385210 -2.296275  2.169809 -1.640033 -2.273093 -1.800326   \n",
      "6    1.679410 -2.385210 -2.296275  2.775741 -1.640033 -2.273093 -1.800326   \n",
      "7    0.774375 -0.989705 -2.296275  2.775741 -1.640033 -1.924965 -1.227519   \n",
      "8    1.255229 -2.385210 -1.239289  2.775741 -1.640033 -2.273093 -1.531865   \n",
      "9    0.820402 -0.489809 -0.844867  2.775741 -1.640033 -0.821701 -1.197037   \n",
      "10  -1.120534  1.514445  1.175679  2.775741 -1.469254  0.492376 -0.419617   \n",
      "11  -0.667017  1.175365  0.999233  1.039708 -1.176059  1.456092 -0.560005   \n",
      "12   0.835938 -1.587999  0.519202  2.775741 -1.180708 -1.366149 -1.089886   \n",
      "13   0.402008 -0.262970 -0.736780  2.230308 -0.967545  0.657959 -0.715461   \n",
      "14   0.010430  0.145181 -1.172546  0.448126 -0.635779  0.503235 -0.468394   \n",
      "15   1.679410 -2.385210 -2.296275  2.775741 -0.942417 -2.273093 -1.380289   \n",
      "16   0.948788 -2.385210 -2.296275  2.775741 -1.082166 -2.273093 -1.486741   \n",
      "17   1.679410 -2.385210 -2.296275  2.775741 -1.387681 -2.273093 -1.790223   \n",
      "18   0.962388 -2.011352 -2.296275  2.775741 -1.460799 -2.273093 -0.967132   \n",
      "19   1.215172 -1.714709 -1.188340  1.498255 -1.527033 -2.273093 -1.098992   \n",
      "20   0.348427  1.514445  0.916751  1.875518 -1.368768  0.094654 -0.363508   \n",
      "21  -1.108677  1.514445  1.544225  1.102984 -1.124329  0.777736 -0.274673   \n",
      "22  -1.162234  1.514445  1.544225  0.757905 -0.788627  1.565833  0.152467   \n",
      "23  -0.784124  1.184614  1.544225 -0.126673 -0.507037  1.437191 -0.075485   \n",
      "24  -0.725298  0.822840  0.741312  2.293803 -0.037301  1.565833 -0.057122   \n",
      "25  -1.205639  1.514445  1.544225  2.775741  0.495934  1.565833  0.875009   \n",
      "26  -1.253143  1.514445  1.544225  0.599280  0.924783  1.565833  1.082028   \n",
      "27  -0.867238  1.514445  1.544225 -0.051504  1.122372  1.565833  0.988173   \n",
      "28  -0.992722  1.514445  1.544225 -0.044310  1.233242  1.565833  1.074021   \n",
      "29  -1.263700  1.514445  1.219812  0.727203  1.370446  1.565833  1.501399   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "659  0.829697 -0.094526 -0.421646  0.095759 -1.640033 -0.461167 -0.924149   \n",
      "660  1.574912 -0.536525 -0.071376 -0.703172 -1.640033 -0.858722 -1.391926   \n",
      "661  1.552201 -1.065387 -0.696646 -0.159320 -1.640033 -0.709545 -1.800326   \n",
      "662 -0.028313  0.164341 -0.382546  0.135494 -1.616438 -0.043129 -0.453191   \n",
      "663 -0.710982  0.856829  0.265943  0.299565 -1.192127  0.459553  0.176740   \n",
      "664 -0.655007  0.710895  1.020962 -0.995240 -0.646964  0.212040  0.134242   \n",
      "665 -1.193786  1.514445  1.544225 -0.015758  0.046212  1.201154  0.876591   \n",
      "666 -1.022467  1.061217  1.096205 -0.830070  0.547619  1.565833  0.680229   \n",
      "667 -0.989324  0.876089  0.852052 -0.607036  0.865000  1.162274  0.917881   \n",
      "668  0.201410 -0.154790  0.298030 -0.533241  0.822342  0.259212 -0.008865   \n",
      "669 -0.246463  0.339136 -0.048772 -0.668014  0.876986  0.643826  0.533372   \n",
      "670  1.679410 -1.652435 -0.980645  1.261101  0.341499 -1.135691 -0.925821   \n",
      "671  1.491049 -1.422137 -1.565043 -0.364043 -0.352045 -1.135542 -0.999147   \n",
      "672  1.269427 -1.111951 -1.375961 -0.435824 -0.839948 -1.367756 -1.035002   \n",
      "673  1.605143 -2.385210 -2.296275  2.775741 -1.265746 -2.273093 -1.800326   \n",
      "674  1.134121 -2.385210 -2.296275  1.434328 -1.426411 -2.273093 -1.800326   \n",
      "675  1.469544 -2.385210 -2.296275  0.538713 -1.592754 -2.273093 -1.800326   \n",
      "676  1.675612 -2.385210 -2.296275  0.952370 -1.640033 -2.273093 -1.800326   \n",
      "677  1.315473 -2.147353 -1.992149  0.675317 -1.640033 -2.273093 -1.800326   \n",
      "678  1.679410 -2.385210 -2.292488  0.499344 -1.640033 -2.273093 -1.800326   \n",
      "679  0.287498 -0.143861 -0.783858  1.646375 -1.640033 -1.251976 -1.128657   \n",
      "680 -0.995218  1.514445  1.544225  2.775741 -1.357518  1.341166  0.206783   \n",
      "681 -0.418542  1.514445  1.544225  0.548235 -0.923677  1.514326 -0.201065   \n",
      "682 -0.360062  1.076219  1.544225 -0.041211 -0.459141  1.126471 -0.243558   \n",
      "683 -0.279844  0.620684  0.597910  0.905859 -0.129043  1.565833 -0.310690   \n",
      "684 -0.276510  0.032630 -0.737944 -0.287318  0.083842  1.056152 -0.313949   \n",
      "685  1.120838 -1.203993 -0.657691 -0.060622  0.046580 -1.397362 -0.806196   \n",
      "686  0.474037 -0.188934 -0.493793  0.827485  0.054680 -0.324316 -0.356538   \n",
      "687 -0.909541  0.854743  0.290822  0.025992  0.231692  0.355293  0.271184   \n",
      "688 -0.519555  1.134404  1.031503  0.049797  0.373751  0.690034  0.490533   \n",
      "\n",
      "           WR       CCI      MASS      open    MAMASS      ADTM    volume  \\\n",
      "0    1.146960 -1.777706 -0.509428  1.158463 -1.040353 -0.255899  2.166225   \n",
      "1    0.700071 -1.511136 -0.250977  1.158463 -0.862182  0.168754  1.673894   \n",
      "2    1.655138 -1.684310  0.068535  1.158463 -0.639501  0.021491 -1.416878   \n",
      "3    1.126868 -1.402416  0.437896  1.158463 -0.358347 -0.665514  2.166225   \n",
      "4    1.655138 -1.354671  0.776542  0.798778 -0.034783 -1.166849  2.166225   \n",
      "5    1.557161 -1.257185  1.061837  0.185242  0.287796 -1.263670  1.134130   \n",
      "6    1.655138 -1.125516  1.317404  0.269204  0.605101 -1.272284  0.788340   \n",
      "7    1.190468 -0.980782  1.569622 -0.592654  0.921322 -1.207498  1.154836   \n",
      "8    1.532900 -0.882884  1.819064  0.089896  1.225373 -0.657886  0.938283   \n",
      "9    1.213470 -0.803263  2.054927 -0.524897  1.516219 -1.009391 -0.028232   \n",
      "10   0.694457 -0.517719  2.054927 -0.134825  1.787775 -0.755776  0.899437   \n",
      "11   0.729758 -0.393874  2.054927  0.253817  2.039168 -0.678359  1.136659   \n",
      "12   1.283280 -0.597523  2.054927 -0.004241  2.053428 -1.068889  0.534723   \n",
      "13   0.806389 -0.676621  2.054927 -0.156632  2.053428 -1.224876 -0.186066   \n",
      "14  -0.120516 -0.380714  2.054927  0.001542  2.053428 -1.268235 -0.456522   \n",
      "15   1.655138 -1.423395  2.054927 -0.132147  2.053428 -1.634281  0.747703   \n",
      "16   0.945312 -1.777706  2.054927 -0.851464  2.053428 -1.774167  0.896035   \n",
      "17   1.655138 -1.777706  2.054927 -1.173742  2.053428 -1.856227 -0.335141   \n",
      "18   1.088250 -1.310930  2.054927 -1.529450  2.053428 -1.865196 -0.034853   \n",
      "19   1.309170 -1.032421  2.054927 -1.140275  2.053428 -1.706401 -0.491951   \n",
      "20   0.715543 -0.588093  2.054927 -1.241263  2.053428 -1.803326 -0.380960   \n",
      "21   0.574714 -0.399117  2.054927 -1.017188  2.053428 -1.744344 -0.577796   \n",
      "22   0.016429 -0.051978  2.054927 -0.797589  2.053428 -1.180336 -0.111112   \n",
      "23   0.230473 -0.031319  2.054927 -0.592906  2.053428 -1.015948 -1.008712   \n",
      "24  -0.022319 -0.164731  1.968833 -1.086470  2.053428 -1.337256 -1.154896   \n",
      "25  -1.297194  0.683456  1.364790 -0.671739  2.053428 -1.081716  0.594193   \n",
      "26  -1.337822  1.063217  0.799501 -0.285326  2.053428 -0.313522  1.055024   \n",
      "27  -1.057907  1.189983  0.128476 -0.046743  1.626282  0.245697  1.242354   \n",
      "28  -1.126163  1.204027 -0.581572 -0.185733  1.077623  0.395525 -0.153185   \n",
      "29  -1.336602  1.330519 -1.231172  0.044380  0.437944  0.444914  1.278580   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "659  0.971328 -1.001110 -0.847448 -2.439253 -0.572827 -0.500143 -1.031644   \n",
      "660  1.623348 -1.035950 -0.974680 -2.439253 -0.679899 -0.960497 -0.986441   \n",
      "661  1.652343 -1.377582 -1.127508 -2.439253 -0.799338 -0.805728 -1.411603   \n",
      "662  0.767280 -0.911562 -1.159475 -2.439253 -0.918911 -0.653881 -1.106987   \n",
      "663  0.097393 -0.194378 -1.078813 -2.439253 -1.008305 -0.418357 -0.352020   \n",
      "664  0.135765  0.007124 -1.087495 -2.439253 -1.077384  0.044808 -0.989369   \n",
      "665 -1.260323  0.493821 -1.086967 -2.439253 -1.118986  0.043467 -0.421434   \n",
      "666 -1.083963  0.750587 -1.057506 -2.439253 -1.133372  0.010766 -1.213327   \n",
      "667 -1.049844  1.137466 -1.029071 -2.439253 -1.116274  0.195881 -0.491745   \n",
      "668 -0.303803  0.685968 -0.952615 -2.421944 -1.080345  0.172300 -0.688023   \n",
      "669 -0.865346  0.841310 -0.899172 -2.439253 -1.049143 -0.251603 -0.937357   \n",
      "670  1.044520 -0.649318 -0.785382 -2.439253 -0.996669 -1.085661 -0.578099   \n",
      "671  1.188156 -0.920704 -0.764740 -2.439253 -0.940701 -1.244873 -1.345905   \n",
      "672  1.275389 -0.991346 -0.719385 -2.439253 -0.881972 -1.154530 -1.291949   \n",
      "673  1.629140 -1.777706 -0.431408 -2.439253 -0.778163 -1.568719  1.053892   \n",
      "674  1.174059 -1.777706 -0.065874 -2.439253 -0.624145 -1.714292  0.513336   \n",
      "675  1.530497 -1.647210  0.349048 -2.439253 -0.407340 -1.828500 -0.636937   \n",
      "676  1.655138 -1.454564  0.764930 -2.439253 -0.138065 -1.865196 -0.709613   \n",
      "677  1.424746 -1.152099  1.127984 -2.439253  0.190684 -1.865196 -0.204416   \n",
      "678  1.655138 -1.063774  1.425395 -2.439253  0.563213 -1.865196 -0.786928   \n",
      "679  1.088268 -0.846077  1.682443 -2.439253  0.930370 -1.865196 -0.202162   \n",
      "680  0.149888 -0.262087  1.939666 -2.439253  1.278714 -1.569420  1.197112   \n",
      "681  0.438673 -0.232908  2.054927 -2.439253  1.577273 -1.070709  0.103239   \n",
      "682  0.107528 -0.245294  2.035382 -2.439253  1.797939 -1.197592 -0.605142   \n",
      "683 -0.319481 -0.378430  2.013774 -2.439253  1.951792 -1.135696 -0.194648   \n",
      "684 -0.316050 -0.045961  1.918322 -2.439253  2.037409 -0.590830 -0.296533   \n",
      "685  0.216803 -0.329000  1.829401 -2.439253  2.053428 -0.781954 -0.926417   \n",
      "686 -0.096583 -0.164655  1.767767 -2.439253  2.033077 -1.153145 -0.006411   \n",
      "687 -0.603923  0.708954  1.687986 -2.439253  1.967080 -1.075711  0.104417   \n",
      "688 -0.801085  1.180965  1.526623 -2.439253  1.878713 -1.018262  0.784497   \n",
      "\n",
      "        money        AR     close  \n",
      "0    2.005205 -0.735627  1.145974  \n",
      "1    2.005205 -0.219956  1.145974  \n",
      "2   -1.608735 -0.927717  0.844781  \n",
      "3    2.005205 -1.025017  1.059666  \n",
      "4    2.005205 -1.337040  0.182208  \n",
      "5    0.876636 -1.337040  0.198111  \n",
      "6    0.567425 -1.337040 -0.224194  \n",
      "7    0.814029 -0.979589  0.174589  \n",
      "8    0.729913 -1.121118 -0.284499  \n",
      "9    0.079339 -0.859486 -0.115223  \n",
      "10   0.896127 -0.583592  0.318141  \n",
      "11   1.202723 -0.771433  0.201766  \n",
      "12   0.656677 -0.807742 -0.298002  \n",
      "13  -0.029152 -0.901849 -0.111840  \n",
      "14  -0.273768 -1.027705  0.008753  \n",
      "15   0.552428 -1.257612 -0.889051  \n",
      "16   0.435508 -1.337040 -1.038251  \n",
      "17  -0.508742 -1.337040 -1.520813  \n",
      "18  -0.289673 -1.316811 -1.127756  \n",
      "19  -0.606109 -1.310356 -1.266092  \n",
      "20  -0.401506 -1.184217 -0.894375  \n",
      "21  -0.616249 -1.009097 -0.847606  \n",
      "22  -0.080321 -0.956003 -0.626517  \n",
      "23  -0.841781 -1.017230 -0.740304  \n",
      "24  -0.925592 -0.767333 -0.732671  \n",
      "25   0.570750 -0.040438 -0.266917  \n",
      "26   0.967215 -0.315627 -0.130111  \n",
      "27   1.063023 -0.467864 -0.158477  \n",
      "28   0.073759  0.081061 -0.114014  \n",
      "29   1.092448  0.558864  0.132685  \n",
      "..        ...       ...       ...  \n",
      "659 -1.608735  0.006789 -2.438783  \n",
      "660 -1.608735 -0.671368 -2.438783  \n",
      "661 -1.608735 -0.921278 -2.438783  \n",
      "662 -1.608735 -0.836549 -2.438783  \n",
      "663 -1.252216 -0.534081 -2.438783  \n",
      "664 -1.608735 -0.174585 -2.438783  \n",
      "665 -1.386765 -0.102718 -2.438783  \n",
      "666 -1.608735  0.420352 -2.438783  \n",
      "667 -1.420702  0.671337 -2.425733  \n",
      "668 -1.608735  0.204547 -2.438783  \n",
      "669 -1.608735  0.497631 -2.438783  \n",
      "670 -1.514902  0.038002 -2.438783  \n",
      "671 -1.608735 -0.007455 -2.438783  \n",
      "672 -1.608735 -0.478757 -2.438783  \n",
      "673 -0.877576 -0.982665 -2.438783  \n",
      "674 -1.208442 -1.072784 -2.438783  \n",
      "675 -1.608735 -1.039844 -2.438783  \n",
      "676 -1.608735 -1.108578 -2.438783  \n",
      "677 -1.608735 -1.232661 -2.438783  \n",
      "678 -1.608735 -1.337040 -2.438783  \n",
      "679 -1.496095 -1.056727 -2.438783  \n",
      "680 -0.663623 -0.600860 -2.438783  \n",
      "681 -1.304229 -0.876163 -2.438783  \n",
      "682 -1.608735 -0.599122 -2.438783  \n",
      "683 -1.562662 -0.366953 -2.438783  \n",
      "684 -1.608735 -0.415989 -2.438783  \n",
      "685 -1.608735 -0.469845 -2.438783  \n",
      "686 -1.395252 -0.253940 -2.438783  \n",
      "687 -1.355481  0.124751 -2.438783  \n",
      "688 -0.855274  0.050358 -2.438783  \n",
      "\n",
      "[689 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f = FeatureSelection()\n",
    "\n",
    "lgbm_res = f.identify_importance_lgbm(data_x,data_y,p=0.9)\n",
    "print(f.columns)\n",
    "print(lgbm_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['open', 'close', 'low', 'ACCER', 'BIAS'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "estimator = LinearSVC()\n",
    "res = f.wrapper_select(data_x=data_x,data_y=data_y,n=5,estimator=estimator)\n",
    "print(f.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ACCER', 'MTR', 'BIAS', 'CCI', 'CYE', 'KBJ', 'RSI', 'RSI6', 'WR',\n",
      "       'MAWR'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "est = LinearSVC(C=0.01,penalty='l1',dual=False)\n",
    "est1 = RandomForestClassifier()\n",
    "e_res = f.embedded_select(data_x=data_x,data_y=data_y,estimator=est1)\n",
    "print(f.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
